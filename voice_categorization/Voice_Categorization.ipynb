{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from torch import nn, optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial. 음성 파일(.wav)을 torchaudio로 읽기\n",
    "음성 파일은 일정한 시간 간격을 가지고 어떤 지점의 음압을 측정한 것으로, 이때 일정한 시간 간격으로 음압을 측정하는 주파수를 sampling rate라고 한다. \\\n",
    "torchaudio를 사용해 음성을 읽으면 음성 데이터와 sampling rate를 반환하는데 아래의 음성에서는 sampling rate가 16000Hz임을 알 수 있다. \\\n",
    "한편, 음성 데이터의 크기를 보면 [1,16000]으로 나와 있는데 1은 채널의 개수 즉, 녹음한 마이크의 개수를 의미하고 16000은 데이터의 길이를 의미한다. sampling rate가 16000Hz인데 길이가 16000이라는 것은 이 음성의 길이가 1초라는 것을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./Data/test/test_00001.wav\"\n",
    "x, sr = torchaudio.load(file_name)\n",
    "print(x.shape, sr)\n",
    "\n",
    "#torch.Size([1,16000]) 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 읽은 음성 데이터를 시각화한 것이다. 가로축은 시간을 세로축은 음압을 의미한다. 음압이 큰 부분에 사람의 목소리가 있음을 추측할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1,figsize=(8,4))\n",
    "ax.plot(x[0,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 다음으로 이 음성 데이터를 딥러닝 모델이 더 잘 이해할 수 있는 spectrogram이나 melspectrogram으로 변환하는 방법에 대해 알아보겠다. \n",
    " \n",
    " 1. 푸리에 변환: 음성 신호에 푸리에 변환을 적용하면 각 진동수 성분이 그 음성에 얼마나 들어있는지 알 수 있다. 쉽게 설명하자면 음성 신호에서 저음과 고음의 분포를 정량적으로 구할 수 있다는 의미이다.\n",
    " 2. STFT (Short Time Fourier Transform): 1초짜리 음성에 푸리에 변환을 적용하면 음성 전체에서 각 진동수 성분이 얼마나 들어있는지를 구할 수 있다. 다만, 음성 속의 단어를 파악하기 위해서는 시간에 따라 변하는 진동수 분포의 양상을 파악해야 한다. 따라서, 음성을 보다 작게 (0.01초 수준) 잘라서 각각의 작은 조각에 푸리에 변환을 적용할 수 있다. 이것을 STFT라고 부르고 일반적으로 이 결과의 L2 Norm을 Spectrogram이라고 부른다. \n",
    " 3. Melspectrogram: Melspectrogram은 spectrogram에 mel-filter를 적용해서 얻을 수 있다. 이는 사람의 청각 기관이 고음에서 주파수의 변화에 덜 민감하고 저음에서 더 민감한 특징을 반영하고 있다. 딥러닝과 사람의 청각 반응은 관련이 없어 보일 수 있으나 음성 처리나 자연어 처리 분야에서도 melspectrogram은 널리 사용되고 있으며, 좋은 성능을 보여주고 있다. 또한 melspectrogram은 spectrogram보다 크기가 작아서 학습 속도 등에서 유리한 점이 있다. \n",
    " \n",
    " torchaudio에서는 다음과 같이 spectrogram과 melspectrogram을 얻을 수 있는 프로세스를 정의할 수 있다. AmplituteToDB는 power 단위의 spectrogram 또는 melspectrogram을 dB(로그) 단위로 변환해준다. 개인적으로 dB 단위가 딥러닝 모델이 이해하기 편한 범위의 값을 제공한다고 생각한다. \n",
    " \n",
    " 1. n_fft: win_length의 크기로 잘린 음성의 작은 조각은 0으로 padding되어 n_fft의 크기로 조절된다. 이후 padding된 조각에 푸리에 변환을 적용한다. 따라서 n_fft는 win_length보다 크거나 같아야 하고, 일반적으로 빠른 학습을 위해 2^n의 값으로 설정된다. \n",
    " 2. win_length: 원래의 음성을 작게 잘랐을 때의 조각의 크기를 의미한다. 자연어 처리 분야에서는 25m의 크기를 기본으로 하고 있으며, 16000Hz인 음성에서는 400에 해당하는 값이다.\n",
    " 3. hop_length: 음성을 작은 조각으로 자를 때의 간격을 의미한다. 즉, 이 길이만큼 옆으로 밀면서 작은 조각을 얻는다. 일반적으로 10ms의 크기를 기본으로 하고 있으며 16000Hz인 음성에서는 160에 해당하는 값이다. \n",
    " 4. n_mels: 적용할 mel filter의 개수를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = nn.Sequential(\n",
    "    AT.Spectrogram(n_fft=512, win_length=400, hop_length=160),\n",
    "    AT.AmplitudeToDB()\n",
    ")\n",
    "\n",
    "mel_spectrogram = nn.Sequential(\n",
    "    AT.MelSpectrogram(sample_rate=sr, n_fft=512, win_length=400, hop_length=160, n_mels=80),\n",
    "    AT.AmplitudeToDB()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 이제 실제 음성 데이터로부터 spectrogram과 melspectrogram을 얻어보겠다. 각각의 크기는 채널을 무시하면 [257,101], [80,101]인데, 101은 시간축 방향 성분의 수, 257과 80은 주파수 방향 성분의 수를 의미한다. n_mel=80이었으므로 melspectrogram의 주파수 방향 성분의 수는 80인 것이다. spectrogram의 경우 (n_fft / 2 + 1)개의 주파수 방향 성분을 얻을 수 있다. \\\n",
    " 주파수 성분은 0Hz부터 sampling rate의 절반 즉, 8000Hz까지를 나타내게 된다. sampling rate의 절반까지밖에 표현하지 못하는 이유는 Nyquist frequency에 대해 알아보면 이해할 수 있을 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = spectrogram(x)\n",
    "mel = mel_spectrogram(x)\n",
    "print(spec.shape, mel.shape)\n",
    "\n",
    "# torch.Size([1,257,101]) torch.Size([1,80,101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 마지막으로 spectorgram과 melspectrogram의 해상력에 대해 설명하겠다. \\\n",
    " win_length가 커질수록 주파수 성분에 대한 해상력은 높아지지만, 즉 더 정밀해지지만, 시간 성분에 대한 해상력은 낮아지게 된다. 즉, 더 정밀한 주파수 분포를 얻을 수 있으나 시간에 따른 주파수 변화를 관찰하기가 어려워진다. \\\n",
    " 반대로 win_length가 작은 경우에는 주파수 성분에 대한 해상력은 낮아지지만, 시간 성분에 대한 해상력은 높아지게 된다. \\\n",
    " 따라서 적절한 값의 win_length를 찾는 것이 중요하다. 또한 n_fft를 키우는 경우 주파수 성분의 수는 증가할지 몰라도 실제 주파수의 해상력은 증가하지 않는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,2,figsize=(8,4))\n",
    "ax[0].pcolor(spec[0])\n",
    "ax[1].pcolor(mel[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Feature 생성: Multi-Resolution Mel-Spectrogram\n",
    " 4가지의 서로 다른 window_length로 다양한 해상력을 가진 mel-spectrogram을 만든다.  \\\n",
    " 이 4개의 mel-spectrogram을 stack한 후, 평균과 표준편차를 정규화한다. \\\n",
    " 또한, 각 주파수에 대하여 보정 값을 더해서 최종 feature를 완성한다.\n",
    " (feature의 크기는 (4,160,160)이다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class MRMS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MRMS, self).__init__()\n",
    "        self.sr, self.n_fft, self.hop, self.pad, self.f_min, self.f_max, self.n_mels = 16000, 2048, 100, 50, 25, 7500, 160\n",
    "        self.tf_0 = self.create_tf(250)\n",
    "        self.tf_1 = self.create_tf(500)\n",
    "        self.tf_2 = self.create_tf(750)\n",
    "        self.tf_3 = self.create_tf(1000)\n",
    "        self.cali = torch.linspace(-0.5, 0.5, steps=160, device=DEVICE).view(1,-1,1)\n",
    "    \n",
    "    def create_tf(self, win_length):\n",
    "        tf = nn.Sequential(\n",
    "            AT.MelSpectrogram(sample_rate=self.sr, n_fft=self.n_fft, win_length=win_length,\n",
    "                              hop_length=self.hop, pad=self.pad, f_min=self.f_min, \n",
    "                              f_max=self.f_max, n_mels=self.n_mels),\n",
    "            AT.AmplitudeToDB()\n",
    "        )\n",
    "        return tf\n",
    "    \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            spec_0 = self.tf_0(x)[0, :, 1:-1]\n",
    "            spec_1 = self.tf_1(x)[0, :, 1:-1]\n",
    "            spec_2 = self.tf_2(x)[0, :, 1:-1]\n",
    "            spec_3 = self.tf_3(x)[0, :, 1:-1]\n",
    "            \n",
    "            out = torch.stack([spec_0, spec_1, spec_2, spec_3])\n",
    "            out = (out - out.mean(dim=[1,2], keepdim=True)) / 20 + self.cali\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = MRMS().to(DEVICE)\n",
    "\n",
    "for file_name in tqdm(np.sort(glob(\"./Data/train/*.wav\"))):\n",
    "    x, _ = torchaudio.load(file_name)\n",
    "    x = x.to(DEVICE)\n",
    "    spec = extractor(x)\n",
    "    name = \"./Cache/train/\" + file_name.split(\"/\")[-1].split(\".\")[0] + \".pt\"\n",
    "    torch.save(spec.to(\"cpu\"), name)\n",
    "\n",
    "for file_name in tqdm(np.sort(glob(\"./Data/test/*.wav\"))):\n",
    "    x, _ = torchaudio.load(file_name)\n",
    "    x = x.to(DEVICE)\n",
    "    spec = extractor(x)\n",
    "    name = \"./Cache/test/\" + file_name.split(\"/\")[-1].split(\".\")[0] + \".pt\"\n",
    "    torch.save(spec.to(\"cpu\"), name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data Augmentation\n",
    " 이 문제를 이미지 탐지 및 분류 문제로 접근해보자. \\\n",
    " 즉, 모델은 mel-spectrogram의 모양을 보고 특정 클래스를 탐지하는 것이다. 따라서, 이미지 분류 및 컴퓨터 비전 분야에서 효과적으로 사용되는 augmentation 기법을 적용하였다. 음성 자체에 적용되는 pitch shifting과 같은 방법보다는 계산적으로도 유리하고 단순한 a) Random Crop과 b) Mixup 기법을 적용하였다. 추가로 Time/Frequency Masking을 적용하였다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, file_list, gt_list, augmentation):\n",
    "        self.file_list = file_list\n",
    "        self.gt_list = gt_list\n",
    "        self.augmentation = augmentation\n",
    "        self.spec_augment = nn.Sequential(\n",
    "            AT.FrequencyMasking(32, False), AT.TimeMasking(12, False), AT.TimeMasking(12, False),\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        with torch.no_grad():\n",
    "            x = torch.load(self.file_list[index])\n",
    "            gt = self.gt_list[index]\n",
    "            \n",
    "            if self.augmentation:\n",
    "                x = self.spec_augment(x)\n",
    "                i,j = random.randrange(64), random.randrange(64)\n",
    "                x = F.pad(x, [32,32,32,32])\n",
    "                x = x[:, i:i+160, j:j+160]\n",
    "                \n",
    "                if random.random() > 0.25:\n",
    "                    mixup_lambda = random.uniform(0.05, 0.25)\n",
    "                    mixup_index = random.randrange(self.__len__())\n",
    "                    mixup_x = torch.load(self.file_list[mixup_index])\n",
    "                    mixup_gt = self.gt_list[mixup_index]\n",
    "                    x = (1 - mixup_lambda) * x + mixup_lambda * mixup_x\n",
    "                    gt = (1 - mixup_lambda) * gt + mixup_lambda * mixup_gt\n",
    "        \n",
    "        return x, gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Set learning parameters and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MODEL = 16\n",
    "N_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "MODEL_FACTOR = 24\n",
    "LEARNING_RATE = 0.2\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0001\n",
    "LOADER_PARAM = {\n",
    "        \"batch_size\": BATCH_SIZE, \"num_workers\": 3, \"pin_memory\": True\n",
    "}\n",
    "\n",
    "train_x = np.sort(glob(\"./Cache/train/*.pt\"))\n",
    "test_x = np.sort(glob(\"./Cache/test/*.pt\"))\n",
    "train_y = torch.tensor(pd.read_csv(\"./Data/train_answer.csv\").to_numpy()[:,1:], dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(CustomDataset(train_x, train_y, augmentation=True), shuffle=True, \n",
    "                          drop_last=True, **LOADER_PARAM)\n",
    "test_loader = DataLoader(CustomDataset(test_x, list(range(10000)), augmentation=False), shuffle=False,\n",
    "                         drop_last=False, **LOADER_PARAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model\n",
    " 최근 이미지 분야에서 매우 효과적인 모델(e.g. DARTS, EfficientNet 등)이 있지만 이 대회에서는 간단하게 VGG와 비슷한 모델을 구성하였다. 아래와 같이 Pooling 방법을 달리한 2가지 모델을 만들었다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bn_relu_conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, ks, n_in, n_out):\n",
    "        super(bn_relu_conv, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(n_in)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "        self.conv = nn.Conv2d(n_in, n_out, kernel_size=ks, padding=ks // 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(self.relu(self.bn(x)))\n",
    "\n",
    "class MAC(nn.Module):\n",
    "    \n",
    "    def __init__(self, pool_method):\n",
    "        super(MAC, self).__init__()\n",
    "        self.factor = MODEL_FACTOR\n",
    "        self.pool = nn.AvgPool2d(2) if pool_method == \"Avg\" else nn.MaxPool2d(2)\n",
    "        self.initialize = nn.Conv2d(4, 2*self.factor, 6, stride=2, padding=2)\n",
    "       \n",
    "        self.lg_0 = nn.Sequential(\n",
    "            bn_relu_conv(1, 2*self.factor, self.factor),\n",
    "            bn_relu_conv(3, self.factor, self.factor),\n",
    "            bn_relu_conv(1, self.factor, self.factor),\n",
    "            bn_relu_conv(3, self.factor, self.factor)\n",
    "        )\n",
    "        self.lg_1 = nn.Sequential(\n",
    "            bn_relu_conv(1, self.factor, 2*self.factor),\n",
    "            bn_relu_conv(3, 2*self.factor, 2*self.factor),\n",
    "            bn_relu_conv(1, 2*self.factor, 2*self.factor),\n",
    "            bn_relu_conv(3, 2*self.factor, 2*self.factor)\n",
    "        )\n",
    "        self.lg_2 = nn.Sequential(\n",
    "            bn_relu_conv(1, 2*self.factor, 4*self.factor),\n",
    "            bn_relu_conv(3, 4*self.factor, 4*self.factor),\n",
    "            bn_relu_conv(1, 4*self.factor, 4*self.factor),\n",
    "            bn_relu_conv(3, 4*self.factor, 4*self.factor)\n",
    "        )\n",
    "        self.lg_3 = nn.Sequential(\n",
    "            bn_relu_conv(1, 4*self.factor, 6*self.factor),\n",
    "            bn_relu_conv(3, 6*self.factor, 6*self.factor),\n",
    "            bn_relu_conv(1, 6*self.factor, 6*self.factor),\n",
    "            bn_relu_conv(3, 6*self.factor, 6*self.factor)\n",
    "        )\n",
    "        self.lg_4 = nn.Sequential(\n",
    "            bn_relu_conv(1, 6*self.factor, 8*self.factor),\n",
    "            bn_relu_conv(3, 8*self.factor, 8*self.factor),\n",
    "            bn_relu_conv(1, 8*self.factor, 8*self.factor),\n",
    "            bn_relu_conv(3, 8*self.factor, 8*self.factor)\n",
    "        )\n",
    "        \n",
    "        self.finalize = nn.Sequential(\n",
    "            bn_relu_conv(1, 8*self.factor, 16*self.factor),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        ) if pool_method == \"Avg\" else nn.Sequential(\n",
    "            bn_relu_conv(1, 8*self.factor, 16*self.factor),\n",
    "           nn.AdaptiveMaxPool2d(1)\n",
    "        )\n",
    "        \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(16*self.factor, 8*self.factor),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.BatchNorm1d(8*self.factor),\n",
    "            nn.LeakyReLU(0.25),\n",
    "            nn.Linear(8*self.factor, 30)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.initialize(x)\n",
    "        x = self.pool(self.lg_0(x))\n",
    "        x = self.pool(self.lg_1(x))\n",
    "        x = self.pool(self.lg_2(x))\n",
    "        x = self.pool(self.lg_3(x))\n",
    "        x = self.finalize(self.lg_4(x))\n",
    "        x = self.dense(x.view(x.shape[0], -1))\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Train\n",
    " 오차 함수로는 KL-Divergence,Optimizer로는 SGD를 사용했다. 이는 다양한 실험에서 SGD가 Adam에 비해 더 좋은 값으로 수렴함을 확인할 수 있었기 때문이다. 또한, PyTorch의 AMP(Automatic Mixed Precision)을 이용해 메모리를 절약하고 학습 시간을 단축했다. \\\n",
    " Max Pooling을 적용한 모델 8개 및 Avg Pooling을 적용한 모델 8개를 전체 학습 데이터에 대해 학습하였고 총 16개의 결과를 평균하여 결과를 제출하였다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_no, pool_method):\n",
    "    model = MAC(pool_method).to(DEVICE)\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM,\n",
    "                          weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=N_EPOCH, eta_min=LEARNING_RATE / 40)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for epoch in range(N_EPOCH):\n",
    "        model.train()\n",
    "        running_loss, running_count = 0., 0\n",
    "        \n",
    "        for i, (xx,yy) in tqdm(enumerate(train_loader), leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast():\n",
    "                xx,yy = xx.to(DEVICE), yy.to(DEVICE)\n",
    "                pred = model(xx)\n",
    "                loss = criterion(pred, yy)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimzier)\n",
    "            scaler.update()\n",
    "            scheduler.step(epoch + i / len(train_loader))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                running_loss += loss.item() * len(yy)\n",
    "                running_count += len(yy)\n",
    "        print(\"{}Pool Model {:01d} Epoch {:03d} | Train {:7.5f}\"\n",
    "              .format(pool_method, model_no+1, epoch+1, running_loss/running_count))\n",
    "    \n",
    "    model.eval()\n",
    "    prediction = torch.zeros((10000,30), dtype=torch.float32)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (xx,_) in enumerate(test_loader):\n",
    "            xx = xx.to(DEVICE)\n",
    "            pred = model(xx).detach().exp().to(\"cpu\")\n",
    "            prediction[BATCH_SIZE * idx:min(BATCH_SIZE * (idx+1), len(prediction))] = pred[:,:]\n",
    "    \n",
    "    df = pd.read_csv(\"./Data/submission.csv\")\n",
    "    df.iloc[:,1:] = prediction.numpy()\n",
    "    df.to_csv(\"./SubResult/{}{:01d}.csv\".format(pool_method, model_no+1), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(N_MODEL // 2):\n",
    "    train(m, \"Avg\")\n",
    "    train(m, \"Max\")\n",
    "\n",
    "out = np.zeros((10000, 30), dtype=np.float32)\n",
    "file_list = glob(\"./SubResult/*.csv\")\n",
    "\n",
    "for file_name in file_list:\n",
    "    print(file_name)\n",
    "    out += pd.read_csv(file_name).to_numpy()[:,1:]\n",
    "out /= len(file_list)\n",
    "\n",
    "df = pd.read_csv(\"./data/submission.csv\")\n",
    "df.iloc[:,1:] = out\n",
    "df.to_csv(\"./PredictionFinal.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Result\n",
    " Public LB: 0.34680 \\\n",
    " Private LB: 0.34732"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
