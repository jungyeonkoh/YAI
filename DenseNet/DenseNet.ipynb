{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11d146a06dcc4d0388ca9bbe37505f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6680a9e6e8a14b4b8a8dc1f03c2dbac8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7996161a043b4981a47876f4fd148c57",
              "IPY_MODEL_01155ea4ef4d47bfb69ee6e984002763"
            ]
          }
        },
        "6680a9e6e8a14b4b8a8dc1f03c2dbac8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7996161a043b4981a47876f4fd148c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5075c552564f470c8843edf9573f8c6d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_475f1b88290f4673bfec448530e73622"
          }
        },
        "01155ea4ef4d47bfb69ee6e984002763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fd4dcc5b496545238e8dfcc1c1d344b4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:30&lt;00:00, 5963833.77it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_17c77b7c97a14863b50f859daac4237e"
          }
        },
        "5075c552564f470c8843edf9573f8c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "475f1b88290f4673bfec448530e73622": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd4dcc5b496545238e8dfcc1c1d344b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "17c77b7c97a14863b50f859daac4237e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnvU6gVZuif8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils import data as D\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import torchsummary\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6BUvi8zvG5G",
        "outputId": "556f40da-a1c3-4bc0-dece-b691c9da504d"
      },
      "source": [
        "print(torch.__version__)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HziN2jcDvODW"
      },
      "source": [
        "# Hyperparameters\n",
        "random_seed = 10\n",
        "batch_size = 64\n",
        "validation_ratio = 0.1\n",
        "learning_rate = 0.1 # learning rate decay\n",
        "num_epoch = 30 # originally, 300"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "11d146a06dcc4d0388ca9bbe37505f2c",
            "6680a9e6e8a14b4b8a8dc1f03c2dbac8",
            "7996161a043b4981a47876f4fd148c57",
            "01155ea4ef4d47bfb69ee6e984002763",
            "5075c552564f470c8843edf9573f8c6d",
            "475f1b88290f4673bfec448530e73622",
            "fd4dcc5b496545238e8dfcc1c1d344b4",
            "17c77b7c97a14863b50f859daac4237e"
          ]
        },
        "id": "BvUrV3dGv-zY",
        "outputId": "5bc967ad-1183-4b79-8099-db84446d28f0"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2470, 0.2435, 0.2616))])\n",
        "                           \n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2470, 0.2435, 0.2616))])                    \n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform_train)\n",
        "valid_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform_test)\n",
        "test_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform_test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11d146a06dcc4d0388ca9bbe37505f2c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VECmcDJaxgv6"
      },
      "source": [
        "num_train = len(train_set)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(validation_ratio * num_train))\n",
        "\n",
        "np.random.seed(random_seed)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set, batch_size=batch_size, sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_set, batch_size=batch_size, sampler=valid_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size=batch_size)\n",
        "\n",
        "classes = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNDl5h2W3ASL"
      },
      "source": [
        "Basic Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_eZBXkYx0cu"
      },
      "source": [
        "# composite function\n",
        "class bn_relu_conv(nn.Module):\n",
        "  def __init__(self, nin, nout, kernel_size, stride, padding, bias=False):\n",
        "    super(bn_relu_conv, self).__init__()\n",
        "    self.batch_norm = nn.BatchNorm2d(nin)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.conv = nn.Conv2d(nin, nout, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.batch_norm(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.conv(out)\n",
        "    return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_rc3G7Rz3PP"
      },
      "source": [
        "# bottleneck\n",
        "class bottleneck_layer(nn.Sequential):\n",
        "  def __init__(self, nin, growth_rate, drop_rate=0.2):\n",
        "    super(bottleneck_layer, self).__init__()\n",
        "    self.add_module(\"conv_1x1\", bn_relu_conv(nin=nin, nout=growth_rate*4, kernel_size=1, stride=1, padding=0, bias=False))\n",
        "    self.add_module(\"conv 3x3\", bn_relu_conv(nin=growth_rate*4, nout=growth_rate, kernel_size=3, stride=1, padding=1, bias=False))\n",
        "    self.drop_rate = drop_rate\n",
        "  \n",
        "  def forward(self, x):\n",
        "    bottleneck_output = super(bottleneck_layer, self).forward(x)\n",
        "    # inspired by stochastic depth of ResNet\n",
        "    if self.drop_rate > 0:\n",
        "      bottleneck_output = F.dropout(bottleneck_output, p=self.drop_rate, training=self.training)\n",
        "    \n",
        "    # concatenation instead of addition of ResNet\n",
        "    bottleneck_output = torch.cat((x, bottleneck_output), 1)\n",
        "    return bottleneck_output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1Hjh839139I"
      },
      "source": [
        "# transition\n",
        "class transition_layer(nn.Sequential):\n",
        "  def __init__(self, nin, theta=0.5):\n",
        "    super(transition_layer, self).__init__()\n",
        "    self.add_module(\"conv 1x1\", bn_relu_conv(nin=nin, nout=int(nin*theta), kernel_size=1, stride=1, padding=0, bias=False))\n",
        "    self.add_module(\"avg_pool 2x2\", nn.AvgPool2d(kernel_size=2, stride=2, padding=0))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo1iHw072Zur"
      },
      "source": [
        "# dense block\n",
        "class DenseBlock(nn.Sequential):\n",
        "  def __init__(self, nin, num_bottleneck_layers, growth_rate, drop_rate=0.2):\n",
        "    super(DenseBlock, self).__init__()\n",
        "\n",
        "    for i in range(num_bottleneck_layers):\n",
        "      nin_bottleneck_layer = nin + growth_rate * i\n",
        "      self.add_module(\"bottleneck_layer_%d\" %i, bottleneck_layer(nin=nin_bottleneck_layer, growth_rate=growth_rate, drop_rate=drop_rate))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cnOH13v3Clk"
      },
      "source": [
        "DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCXrfAC229Lv"
      },
      "source": [
        "class DenseNet(nn.Module):\n",
        "  def __init__(self, growth_rate=12, num_layers=100, theta=0.5, drop_rate=0.2, num_classes=10):\n",
        "    super(DenseNet, self).__init__()\n",
        "    assert (num_layers - 4) % 6 == 0 \n",
        "\n",
        "    num_bottleneck_layers = (num_layers - 4) // 6\n",
        "    \n",
        "    self.dense_init = nn.Conv2d(3, growth_rate*2, kernel_size=3, stride=1, padding=1, bias=True)\n",
        "    \n",
        "    self.dense_block_1 = DenseBlock(nin=growth_rate*2, num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "    nin_transition_layer_1 = (growth_rate) * 2 + (growth_rate * num_bottleneck_layers)\n",
        "    self.transition_layer_1 = transition_layer(nin=nin_transition_layer_1, theta=theta)\n",
        "\n",
        "    self.dense_block_2 = DenseBlock(nin=int(nin_transition_layer_1*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "    nin_transition_layer_2 = int(nin_transition_layer_1*theta) + (growth_rate * num_bottleneck_layers)\n",
        "    self.transition_layer_2 = transition_layer(nin=nin_transition_layer_2, theta=theta)\n",
        "\n",
        "    self.dense_block_3 = DenseBlock(nin=int(nin_transition_layer_2*theta), num_bottleneck_layers=num_bottleneck_layers, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "    nin_fc_layer = int(nin_transition_layer_2 * theta) + (growth_rate * num_bottleneck_layers)\n",
        "    self.fc_layer = nn.Linear(nin_fc_layer, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    dense_init_output = self.dense_init(x)\n",
        "\n",
        "    dense_block_1_output = self.dense_block_1(dense_init_output)\n",
        "    transition_layer_1_output = self.transition_layer_1(dense_block_1_output)\n",
        "\n",
        "    dense_block_2_output = self.dense_block_2(transition_layer_1_output)\n",
        "    transition_layer_2_output = self.transition_layer_2(dense_block_2_output)\n",
        "\n",
        "    dense_block_3_output = self.dense_block_3(transition_layer_2_output)\n",
        "    global_avg_pool_output = F.adaptive_avg_pool2d(dense_block_3_output, (1,1))\n",
        "    global_avg_pool_output_flat = global_avg_pool_output.view(global_avg_pool_output.size(0), -1)\n",
        "\n",
        "    output = self.fc_layer(global_avg_pool_output_flat)\n",
        "    return output"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-NIUm166I-X"
      },
      "source": [
        "def DenseNetBC_100_12():\n",
        "  return DenseNet(growth_rate=12, num_layers=100)\n",
        "def DenseNetBC_250_24():\n",
        "  return DenseNet(growth_rate=24, num_layers=250)\n",
        "def DenseNetBC_190_40():\n",
        "  return DenseNet(growth_rate=40, num_layers=190)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqnox7xe6dOw",
        "outputId": "ae4e2a47-6b6e-488d-8496-047061cc7fb8"
      },
      "source": [
        "net = DenseNetBC_100_12()\n",
        "net.to(device)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseNet(\n",
              "  (dense_init): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (dense_block_1): DenseBlock(\n",
              "    (bottleneck_layer_0): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_1): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_2): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_3): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_4): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_5): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_6): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_7): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_8): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_9): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_10): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_11): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_12): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_13): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_14): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_15): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transition_layer_1): transition_layer(\n",
              "    (conv 1x1): bn_relu_conv(\n",
              "      (batch_norm): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(216, 108, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (avg_pool 2x2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "  )\n",
              "  (dense_block_2): DenseBlock(\n",
              "    (bottleneck_layer_0): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_1): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_2): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(132, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_3): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_4): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(156, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_5): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(168, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(168, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_6): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(180, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_7): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_8): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(204, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(204, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_9): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(216, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(216, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_10): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(228, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(228, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_11): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_12): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(252, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(252, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_13): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(264, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(264, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_14): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(276, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_15): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transition_layer_2): transition_layer(\n",
              "    (conv 1x1): bn_relu_conv(\n",
              "      (batch_norm): BatchNorm2d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv): Conv2d(300, 150, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (avg_pool 2x2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "  )\n",
              "  (dense_block_3): DenseBlock(\n",
              "    (bottleneck_layer_0): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(150, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_1): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(162, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_2): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(174, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_3): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(186, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_4): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(198, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_5): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(210, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_6): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(222, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_7): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(234, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(234, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_8): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(246, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(246, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_9): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(258, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_10): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(270, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_11): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(282, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(282, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_12): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(294, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(294, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_13): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(306, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(306, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_14): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(318, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(318, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bottleneck_layer_15): bottleneck_layer(\n",
              "      (conv_1x1): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(330, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(330, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      )\n",
              "      (conv 3x3): bn_relu_conv(\n",
              "        (batch_norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc_layer): Linear(in_features=342, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKBJ1GR26gZp",
        "outputId": "cd95172b-0505-40fe-f217-6f13c66f9291"
      },
      "source": [
        "torchsummary.summary(net, (3,32,32))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 24, 32, 32]             672\n",
            "       BatchNorm2d-2           [-1, 24, 32, 32]              48\n",
            "              ReLU-3           [-1, 24, 32, 32]               0\n",
            "            Conv2d-4           [-1, 48, 32, 32]           1,152\n",
            "      bn_relu_conv-5           [-1, 48, 32, 32]               0\n",
            "       BatchNorm2d-6           [-1, 48, 32, 32]              96\n",
            "              ReLU-7           [-1, 48, 32, 32]               0\n",
            "            Conv2d-8           [-1, 12, 32, 32]           5,184\n",
            "      bn_relu_conv-9           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-10           [-1, 36, 32, 32]              72\n",
            "             ReLU-11           [-1, 36, 32, 32]               0\n",
            "           Conv2d-12           [-1, 48, 32, 32]           1,728\n",
            "     bn_relu_conv-13           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-14           [-1, 48, 32, 32]              96\n",
            "             ReLU-15           [-1, 48, 32, 32]               0\n",
            "           Conv2d-16           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-17           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-18           [-1, 48, 32, 32]              96\n",
            "             ReLU-19           [-1, 48, 32, 32]               0\n",
            "           Conv2d-20           [-1, 48, 32, 32]           2,304\n",
            "     bn_relu_conv-21           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-22           [-1, 48, 32, 32]              96\n",
            "             ReLU-23           [-1, 48, 32, 32]               0\n",
            "           Conv2d-24           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-25           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-26           [-1, 60, 32, 32]             120\n",
            "             ReLU-27           [-1, 60, 32, 32]               0\n",
            "           Conv2d-28           [-1, 48, 32, 32]           2,880\n",
            "     bn_relu_conv-29           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-30           [-1, 48, 32, 32]              96\n",
            "             ReLU-31           [-1, 48, 32, 32]               0\n",
            "           Conv2d-32           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-33           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-34           [-1, 72, 32, 32]             144\n",
            "             ReLU-35           [-1, 72, 32, 32]               0\n",
            "           Conv2d-36           [-1, 48, 32, 32]           3,456\n",
            "     bn_relu_conv-37           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-38           [-1, 48, 32, 32]              96\n",
            "             ReLU-39           [-1, 48, 32, 32]               0\n",
            "           Conv2d-40           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-41           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-42           [-1, 84, 32, 32]             168\n",
            "             ReLU-43           [-1, 84, 32, 32]               0\n",
            "           Conv2d-44           [-1, 48, 32, 32]           4,032\n",
            "     bn_relu_conv-45           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-46           [-1, 48, 32, 32]              96\n",
            "             ReLU-47           [-1, 48, 32, 32]               0\n",
            "           Conv2d-48           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-49           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-50           [-1, 96, 32, 32]             192\n",
            "             ReLU-51           [-1, 96, 32, 32]               0\n",
            "           Conv2d-52           [-1, 48, 32, 32]           4,608\n",
            "     bn_relu_conv-53           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-54           [-1, 48, 32, 32]              96\n",
            "             ReLU-55           [-1, 48, 32, 32]               0\n",
            "           Conv2d-56           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-57           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-58          [-1, 108, 32, 32]             216\n",
            "             ReLU-59          [-1, 108, 32, 32]               0\n",
            "           Conv2d-60           [-1, 48, 32, 32]           5,184\n",
            "     bn_relu_conv-61           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-62           [-1, 48, 32, 32]              96\n",
            "             ReLU-63           [-1, 48, 32, 32]               0\n",
            "           Conv2d-64           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-65           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-66          [-1, 120, 32, 32]             240\n",
            "             ReLU-67          [-1, 120, 32, 32]               0\n",
            "           Conv2d-68           [-1, 48, 32, 32]           5,760\n",
            "     bn_relu_conv-69           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-70           [-1, 48, 32, 32]              96\n",
            "             ReLU-71           [-1, 48, 32, 32]               0\n",
            "           Conv2d-72           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-73           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-74          [-1, 132, 32, 32]             264\n",
            "             ReLU-75          [-1, 132, 32, 32]               0\n",
            "           Conv2d-76           [-1, 48, 32, 32]           6,336\n",
            "     bn_relu_conv-77           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-78           [-1, 48, 32, 32]              96\n",
            "             ReLU-79           [-1, 48, 32, 32]               0\n",
            "           Conv2d-80           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-81           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-82          [-1, 144, 32, 32]             288\n",
            "             ReLU-83          [-1, 144, 32, 32]               0\n",
            "           Conv2d-84           [-1, 48, 32, 32]           6,912\n",
            "     bn_relu_conv-85           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-86           [-1, 48, 32, 32]              96\n",
            "             ReLU-87           [-1, 48, 32, 32]               0\n",
            "           Conv2d-88           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-89           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-90          [-1, 156, 32, 32]             312\n",
            "             ReLU-91          [-1, 156, 32, 32]               0\n",
            "           Conv2d-92           [-1, 48, 32, 32]           7,488\n",
            "     bn_relu_conv-93           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-94           [-1, 48, 32, 32]              96\n",
            "             ReLU-95           [-1, 48, 32, 32]               0\n",
            "           Conv2d-96           [-1, 12, 32, 32]           5,184\n",
            "     bn_relu_conv-97           [-1, 12, 32, 32]               0\n",
            "      BatchNorm2d-98          [-1, 168, 32, 32]             336\n",
            "             ReLU-99          [-1, 168, 32, 32]               0\n",
            "          Conv2d-100           [-1, 48, 32, 32]           8,064\n",
            "    bn_relu_conv-101           [-1, 48, 32, 32]               0\n",
            "     BatchNorm2d-102           [-1, 48, 32, 32]              96\n",
            "            ReLU-103           [-1, 48, 32, 32]               0\n",
            "          Conv2d-104           [-1, 12, 32, 32]           5,184\n",
            "    bn_relu_conv-105           [-1, 12, 32, 32]               0\n",
            "     BatchNorm2d-106          [-1, 180, 32, 32]             360\n",
            "            ReLU-107          [-1, 180, 32, 32]               0\n",
            "          Conv2d-108           [-1, 48, 32, 32]           8,640\n",
            "    bn_relu_conv-109           [-1, 48, 32, 32]               0\n",
            "     BatchNorm2d-110           [-1, 48, 32, 32]              96\n",
            "            ReLU-111           [-1, 48, 32, 32]               0\n",
            "          Conv2d-112           [-1, 12, 32, 32]           5,184\n",
            "    bn_relu_conv-113           [-1, 12, 32, 32]               0\n",
            "     BatchNorm2d-114          [-1, 192, 32, 32]             384\n",
            "            ReLU-115          [-1, 192, 32, 32]               0\n",
            "          Conv2d-116           [-1, 48, 32, 32]           9,216\n",
            "    bn_relu_conv-117           [-1, 48, 32, 32]               0\n",
            "     BatchNorm2d-118           [-1, 48, 32, 32]              96\n",
            "            ReLU-119           [-1, 48, 32, 32]               0\n",
            "          Conv2d-120           [-1, 12, 32, 32]           5,184\n",
            "    bn_relu_conv-121           [-1, 12, 32, 32]               0\n",
            "     BatchNorm2d-122          [-1, 204, 32, 32]             408\n",
            "            ReLU-123          [-1, 204, 32, 32]               0\n",
            "          Conv2d-124           [-1, 48, 32, 32]           9,792\n",
            "    bn_relu_conv-125           [-1, 48, 32, 32]               0\n",
            "     BatchNorm2d-126           [-1, 48, 32, 32]              96\n",
            "            ReLU-127           [-1, 48, 32, 32]               0\n",
            "          Conv2d-128           [-1, 12, 32, 32]           5,184\n",
            "    bn_relu_conv-129           [-1, 12, 32, 32]               0\n",
            "     BatchNorm2d-130          [-1, 216, 32, 32]             432\n",
            "            ReLU-131          [-1, 216, 32, 32]               0\n",
            "          Conv2d-132          [-1, 108, 32, 32]          23,328\n",
            "    bn_relu_conv-133          [-1, 108, 32, 32]               0\n",
            "       AvgPool2d-134          [-1, 108, 16, 16]               0\n",
            "     BatchNorm2d-135          [-1, 108, 16, 16]             216\n",
            "            ReLU-136          [-1, 108, 16, 16]               0\n",
            "          Conv2d-137           [-1, 48, 16, 16]           5,184\n",
            "    bn_relu_conv-138           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-139           [-1, 48, 16, 16]              96\n",
            "            ReLU-140           [-1, 48, 16, 16]               0\n",
            "          Conv2d-141           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-142           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-143          [-1, 120, 16, 16]             240\n",
            "            ReLU-144          [-1, 120, 16, 16]               0\n",
            "          Conv2d-145           [-1, 48, 16, 16]           5,760\n",
            "    bn_relu_conv-146           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-147           [-1, 48, 16, 16]              96\n",
            "            ReLU-148           [-1, 48, 16, 16]               0\n",
            "          Conv2d-149           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-150           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-151          [-1, 132, 16, 16]             264\n",
            "            ReLU-152          [-1, 132, 16, 16]               0\n",
            "          Conv2d-153           [-1, 48, 16, 16]           6,336\n",
            "    bn_relu_conv-154           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-155           [-1, 48, 16, 16]              96\n",
            "            ReLU-156           [-1, 48, 16, 16]               0\n",
            "          Conv2d-157           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-158           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-159          [-1, 144, 16, 16]             288\n",
            "            ReLU-160          [-1, 144, 16, 16]               0\n",
            "          Conv2d-161           [-1, 48, 16, 16]           6,912\n",
            "    bn_relu_conv-162           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-163           [-1, 48, 16, 16]              96\n",
            "            ReLU-164           [-1, 48, 16, 16]               0\n",
            "          Conv2d-165           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-166           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-167          [-1, 156, 16, 16]             312\n",
            "            ReLU-168          [-1, 156, 16, 16]               0\n",
            "          Conv2d-169           [-1, 48, 16, 16]           7,488\n",
            "    bn_relu_conv-170           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-171           [-1, 48, 16, 16]              96\n",
            "            ReLU-172           [-1, 48, 16, 16]               0\n",
            "          Conv2d-173           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-174           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-175          [-1, 168, 16, 16]             336\n",
            "            ReLU-176          [-1, 168, 16, 16]               0\n",
            "          Conv2d-177           [-1, 48, 16, 16]           8,064\n",
            "    bn_relu_conv-178           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-179           [-1, 48, 16, 16]              96\n",
            "            ReLU-180           [-1, 48, 16, 16]               0\n",
            "          Conv2d-181           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-182           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-183          [-1, 180, 16, 16]             360\n",
            "            ReLU-184          [-1, 180, 16, 16]               0\n",
            "          Conv2d-185           [-1, 48, 16, 16]           8,640\n",
            "    bn_relu_conv-186           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-187           [-1, 48, 16, 16]              96\n",
            "            ReLU-188           [-1, 48, 16, 16]               0\n",
            "          Conv2d-189           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-190           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-191          [-1, 192, 16, 16]             384\n",
            "            ReLU-192          [-1, 192, 16, 16]               0\n",
            "          Conv2d-193           [-1, 48, 16, 16]           9,216\n",
            "    bn_relu_conv-194           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-195           [-1, 48, 16, 16]              96\n",
            "            ReLU-196           [-1, 48, 16, 16]               0\n",
            "          Conv2d-197           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-198           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-199          [-1, 204, 16, 16]             408\n",
            "            ReLU-200          [-1, 204, 16, 16]               0\n",
            "          Conv2d-201           [-1, 48, 16, 16]           9,792\n",
            "    bn_relu_conv-202           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-203           [-1, 48, 16, 16]              96\n",
            "            ReLU-204           [-1, 48, 16, 16]               0\n",
            "          Conv2d-205           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-206           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-207          [-1, 216, 16, 16]             432\n",
            "            ReLU-208          [-1, 216, 16, 16]               0\n",
            "          Conv2d-209           [-1, 48, 16, 16]          10,368\n",
            "    bn_relu_conv-210           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-211           [-1, 48, 16, 16]              96\n",
            "            ReLU-212           [-1, 48, 16, 16]               0\n",
            "          Conv2d-213           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-214           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-215          [-1, 228, 16, 16]             456\n",
            "            ReLU-216          [-1, 228, 16, 16]               0\n",
            "          Conv2d-217           [-1, 48, 16, 16]          10,944\n",
            "    bn_relu_conv-218           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-219           [-1, 48, 16, 16]              96\n",
            "            ReLU-220           [-1, 48, 16, 16]               0\n",
            "          Conv2d-221           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-222           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-223          [-1, 240, 16, 16]             480\n",
            "            ReLU-224          [-1, 240, 16, 16]               0\n",
            "          Conv2d-225           [-1, 48, 16, 16]          11,520\n",
            "    bn_relu_conv-226           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-227           [-1, 48, 16, 16]              96\n",
            "            ReLU-228           [-1, 48, 16, 16]               0\n",
            "          Conv2d-229           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-230           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-231          [-1, 252, 16, 16]             504\n",
            "            ReLU-232          [-1, 252, 16, 16]               0\n",
            "          Conv2d-233           [-1, 48, 16, 16]          12,096\n",
            "    bn_relu_conv-234           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-235           [-1, 48, 16, 16]              96\n",
            "            ReLU-236           [-1, 48, 16, 16]               0\n",
            "          Conv2d-237           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-238           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-239          [-1, 264, 16, 16]             528\n",
            "            ReLU-240          [-1, 264, 16, 16]               0\n",
            "          Conv2d-241           [-1, 48, 16, 16]          12,672\n",
            "    bn_relu_conv-242           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-243           [-1, 48, 16, 16]              96\n",
            "            ReLU-244           [-1, 48, 16, 16]               0\n",
            "          Conv2d-245           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-246           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-247          [-1, 276, 16, 16]             552\n",
            "            ReLU-248          [-1, 276, 16, 16]               0\n",
            "          Conv2d-249           [-1, 48, 16, 16]          13,248\n",
            "    bn_relu_conv-250           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-251           [-1, 48, 16, 16]              96\n",
            "            ReLU-252           [-1, 48, 16, 16]               0\n",
            "          Conv2d-253           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-254           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-255          [-1, 288, 16, 16]             576\n",
            "            ReLU-256          [-1, 288, 16, 16]               0\n",
            "          Conv2d-257           [-1, 48, 16, 16]          13,824\n",
            "    bn_relu_conv-258           [-1, 48, 16, 16]               0\n",
            "     BatchNorm2d-259           [-1, 48, 16, 16]              96\n",
            "            ReLU-260           [-1, 48, 16, 16]               0\n",
            "          Conv2d-261           [-1, 12, 16, 16]           5,184\n",
            "    bn_relu_conv-262           [-1, 12, 16, 16]               0\n",
            "     BatchNorm2d-263          [-1, 300, 16, 16]             600\n",
            "            ReLU-264          [-1, 300, 16, 16]               0\n",
            "          Conv2d-265          [-1, 150, 16, 16]          45,000\n",
            "    bn_relu_conv-266          [-1, 150, 16, 16]               0\n",
            "       AvgPool2d-267            [-1, 150, 8, 8]               0\n",
            "     BatchNorm2d-268            [-1, 150, 8, 8]             300\n",
            "            ReLU-269            [-1, 150, 8, 8]               0\n",
            "          Conv2d-270             [-1, 48, 8, 8]           7,200\n",
            "    bn_relu_conv-271             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-272             [-1, 48, 8, 8]              96\n",
            "            ReLU-273             [-1, 48, 8, 8]               0\n",
            "          Conv2d-274             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-275             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-276            [-1, 162, 8, 8]             324\n",
            "            ReLU-277            [-1, 162, 8, 8]               0\n",
            "          Conv2d-278             [-1, 48, 8, 8]           7,776\n",
            "    bn_relu_conv-279             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-280             [-1, 48, 8, 8]              96\n",
            "            ReLU-281             [-1, 48, 8, 8]               0\n",
            "          Conv2d-282             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-283             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-284            [-1, 174, 8, 8]             348\n",
            "            ReLU-285            [-1, 174, 8, 8]               0\n",
            "          Conv2d-286             [-1, 48, 8, 8]           8,352\n",
            "    bn_relu_conv-287             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-288             [-1, 48, 8, 8]              96\n",
            "            ReLU-289             [-1, 48, 8, 8]               0\n",
            "          Conv2d-290             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-291             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-292            [-1, 186, 8, 8]             372\n",
            "            ReLU-293            [-1, 186, 8, 8]               0\n",
            "          Conv2d-294             [-1, 48, 8, 8]           8,928\n",
            "    bn_relu_conv-295             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-296             [-1, 48, 8, 8]              96\n",
            "            ReLU-297             [-1, 48, 8, 8]               0\n",
            "          Conv2d-298             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-299             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-300            [-1, 198, 8, 8]             396\n",
            "            ReLU-301            [-1, 198, 8, 8]               0\n",
            "          Conv2d-302             [-1, 48, 8, 8]           9,504\n",
            "    bn_relu_conv-303             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-304             [-1, 48, 8, 8]              96\n",
            "            ReLU-305             [-1, 48, 8, 8]               0\n",
            "          Conv2d-306             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-307             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-308            [-1, 210, 8, 8]             420\n",
            "            ReLU-309            [-1, 210, 8, 8]               0\n",
            "          Conv2d-310             [-1, 48, 8, 8]          10,080\n",
            "    bn_relu_conv-311             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-312             [-1, 48, 8, 8]              96\n",
            "            ReLU-313             [-1, 48, 8, 8]               0\n",
            "          Conv2d-314             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-315             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-316            [-1, 222, 8, 8]             444\n",
            "            ReLU-317            [-1, 222, 8, 8]               0\n",
            "          Conv2d-318             [-1, 48, 8, 8]          10,656\n",
            "    bn_relu_conv-319             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-320             [-1, 48, 8, 8]              96\n",
            "            ReLU-321             [-1, 48, 8, 8]               0\n",
            "          Conv2d-322             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-323             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-324            [-1, 234, 8, 8]             468\n",
            "            ReLU-325            [-1, 234, 8, 8]               0\n",
            "          Conv2d-326             [-1, 48, 8, 8]          11,232\n",
            "    bn_relu_conv-327             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-328             [-1, 48, 8, 8]              96\n",
            "            ReLU-329             [-1, 48, 8, 8]               0\n",
            "          Conv2d-330             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-331             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-332            [-1, 246, 8, 8]             492\n",
            "            ReLU-333            [-1, 246, 8, 8]               0\n",
            "          Conv2d-334             [-1, 48, 8, 8]          11,808\n",
            "    bn_relu_conv-335             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-336             [-1, 48, 8, 8]              96\n",
            "            ReLU-337             [-1, 48, 8, 8]               0\n",
            "          Conv2d-338             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-339             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-340            [-1, 258, 8, 8]             516\n",
            "            ReLU-341            [-1, 258, 8, 8]               0\n",
            "          Conv2d-342             [-1, 48, 8, 8]          12,384\n",
            "    bn_relu_conv-343             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-344             [-1, 48, 8, 8]              96\n",
            "            ReLU-345             [-1, 48, 8, 8]               0\n",
            "          Conv2d-346             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-347             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-348            [-1, 270, 8, 8]             540\n",
            "            ReLU-349            [-1, 270, 8, 8]               0\n",
            "          Conv2d-350             [-1, 48, 8, 8]          12,960\n",
            "    bn_relu_conv-351             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-352             [-1, 48, 8, 8]              96\n",
            "            ReLU-353             [-1, 48, 8, 8]               0\n",
            "          Conv2d-354             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-355             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-356            [-1, 282, 8, 8]             564\n",
            "            ReLU-357            [-1, 282, 8, 8]               0\n",
            "          Conv2d-358             [-1, 48, 8, 8]          13,536\n",
            "    bn_relu_conv-359             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-360             [-1, 48, 8, 8]              96\n",
            "            ReLU-361             [-1, 48, 8, 8]               0\n",
            "          Conv2d-362             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-363             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-364            [-1, 294, 8, 8]             588\n",
            "            ReLU-365            [-1, 294, 8, 8]               0\n",
            "          Conv2d-366             [-1, 48, 8, 8]          14,112\n",
            "    bn_relu_conv-367             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-368             [-1, 48, 8, 8]              96\n",
            "            ReLU-369             [-1, 48, 8, 8]               0\n",
            "          Conv2d-370             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-371             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-372            [-1, 306, 8, 8]             612\n",
            "            ReLU-373            [-1, 306, 8, 8]               0\n",
            "          Conv2d-374             [-1, 48, 8, 8]          14,688\n",
            "    bn_relu_conv-375             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-376             [-1, 48, 8, 8]              96\n",
            "            ReLU-377             [-1, 48, 8, 8]               0\n",
            "          Conv2d-378             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-379             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-380            [-1, 318, 8, 8]             636\n",
            "            ReLU-381            [-1, 318, 8, 8]               0\n",
            "          Conv2d-382             [-1, 48, 8, 8]          15,264\n",
            "    bn_relu_conv-383             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-384             [-1, 48, 8, 8]              96\n",
            "            ReLU-385             [-1, 48, 8, 8]               0\n",
            "          Conv2d-386             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-387             [-1, 12, 8, 8]               0\n",
            "     BatchNorm2d-388            [-1, 330, 8, 8]             660\n",
            "            ReLU-389            [-1, 330, 8, 8]               0\n",
            "          Conv2d-390             [-1, 48, 8, 8]          15,840\n",
            "    bn_relu_conv-391             [-1, 48, 8, 8]               0\n",
            "     BatchNorm2d-392             [-1, 48, 8, 8]              96\n",
            "            ReLU-393             [-1, 48, 8, 8]               0\n",
            "          Conv2d-394             [-1, 12, 8, 8]           5,184\n",
            "    bn_relu_conv-395             [-1, 12, 8, 8]               0\n",
            "          Linear-396                   [-1, 10]           3,430\n",
            "================================================================\n",
            "Total params: 768,502\n",
            "Trainable params: 768,502\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 87.35\n",
            "Params size (MB): 2.93\n",
            "Estimated Total Size (MB): 90.30\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5TO32-M7JMo"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNz70ej363NI",
        "outputId": "cff9c9a2-32d8-4b4e-8f7d-0a8af30db295"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# stochastic gradient descent\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9) \n",
        "# learning rate decay\n",
        "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[int(num_epoch*0.5), int(num_epoch*0.75)])\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "  lr_scheduler.step()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    if i % show_period == show_period - 1:\n",
        "      print(\"(Epoch, batch): \" + str((epoch+1, (i+1) * batch_size)) + \" Loss: \" + str(running_loss / show_period))\n",
        "      running_loss = 0.0\n",
        "    \n",
        "  \n",
        "  # validation \n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for i, data in enumerate(valid_loader, 0):\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    outputs  = net(inputs)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "  \n",
        "  print(\"Epoch: \" + str(epoch+1) +  \" Accuracy: \" + str(100 * correct / total))\n",
        "\n",
        "print(\"Finished Training\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Epoch, batch): (1, 6400) Loss: 0.6431474420428276\n",
            "(Epoch, batch): (1, 12800) Loss: 0.6405600637197495\n",
            "(Epoch, batch): (1, 19200) Loss: 0.647274187207222\n",
            "(Epoch, batch): (1, 25600) Loss: 0.6351401954889297\n",
            "(Epoch, batch): (1, 32000) Loss: 0.6190198501944542\n",
            "(Epoch, batch): (1, 38400) Loss: 0.6107663002610206\n",
            "(Epoch, batch): (1, 44800) Loss: 0.5876800894737244\n",
            "Epoch: 1 Accuracy: 75.58\n",
            "(Epoch, batch): (2, 6400) Loss: 0.6120308634638786\n",
            "(Epoch, batch): (2, 12800) Loss: 0.5869823995232583\n",
            "(Epoch, batch): (2, 19200) Loss: 0.5908719611167907\n",
            "(Epoch, batch): (2, 25600) Loss: 0.5688532775640488\n",
            "(Epoch, batch): (2, 32000) Loss: 0.5668252521753311\n",
            "(Epoch, batch): (2, 38400) Loss: 0.5603009551763535\n",
            "(Epoch, batch): (2, 44800) Loss: 0.5520080584287643\n",
            "Epoch: 2 Accuracy: 75.78\n",
            "(Epoch, batch): (3, 6400) Loss: 0.5714077201485633\n",
            "(Epoch, batch): (3, 12800) Loss: 0.5297363828122615\n",
            "(Epoch, batch): (3, 19200) Loss: 0.5473501151800155\n",
            "(Epoch, batch): (3, 25600) Loss: 0.5537824577093124\n",
            "(Epoch, batch): (3, 32000) Loss: 0.5551952350139618\n",
            "(Epoch, batch): (3, 38400) Loss: 0.49857685759663584\n",
            "(Epoch, batch): (3, 44800) Loss: 0.528187717795372\n",
            "Epoch: 3 Accuracy: 75.78\n",
            "(Epoch, batch): (4, 6400) Loss: 0.5175016760826111\n",
            "(Epoch, batch): (4, 12800) Loss: 0.5037252420186996\n",
            "(Epoch, batch): (4, 19200) Loss: 0.5021505978703499\n",
            "(Epoch, batch): (4, 25600) Loss: 0.5182982233166694\n",
            "(Epoch, batch): (4, 32000) Loss: 0.5054555900394917\n",
            "(Epoch, batch): (4, 38400) Loss: 0.4816534240543842\n",
            "(Epoch, batch): (4, 44800) Loss: 0.4816287013888359\n",
            "Epoch: 4 Accuracy: 78.42\n",
            "(Epoch, batch): (5, 6400) Loss: 0.4675766526162624\n",
            "(Epoch, batch): (5, 12800) Loss: 0.4563541653752327\n",
            "(Epoch, batch): (5, 19200) Loss: 0.4881057758629322\n",
            "(Epoch, batch): (5, 25600) Loss: 0.46619865447282793\n",
            "(Epoch, batch): (5, 32000) Loss: 0.4659401075541973\n",
            "(Epoch, batch): (5, 38400) Loss: 0.44393534675240515\n",
            "(Epoch, batch): (5, 44800) Loss: 0.4806025220453739\n",
            "Epoch: 5 Accuracy: 78.94\n",
            "(Epoch, batch): (6, 6400) Loss: 0.48371978759765627\n",
            "(Epoch, batch): (6, 12800) Loss: 0.45235755294561386\n",
            "(Epoch, batch): (6, 19200) Loss: 0.4423519304394722\n",
            "(Epoch, batch): (6, 25600) Loss: 0.45863718271255494\n",
            "(Epoch, batch): (6, 32000) Loss: 0.4347879645228386\n",
            "(Epoch, batch): (6, 38400) Loss: 0.4276171900331974\n",
            "(Epoch, batch): (6, 44800) Loss: 0.4368839193880558\n",
            "Epoch: 6 Accuracy: 79.18\n",
            "(Epoch, batch): (7, 6400) Loss: 0.39678111717104914\n",
            "(Epoch, batch): (7, 12800) Loss: 0.4323564052581787\n",
            "(Epoch, batch): (7, 19200) Loss: 0.42319383800029753\n",
            "(Epoch, batch): (7, 25600) Loss: 0.44220602095127104\n",
            "(Epoch, batch): (7, 32000) Loss: 0.4193100170791149\n",
            "(Epoch, batch): (7, 38400) Loss: 0.42097374960780143\n",
            "(Epoch, batch): (7, 44800) Loss: 0.4116896502673626\n",
            "Epoch: 7 Accuracy: 82.58\n",
            "(Epoch, batch): (8, 6400) Loss: 0.36805599853396415\n",
            "(Epoch, batch): (8, 12800) Loss: 0.38643025636672973\n",
            "(Epoch, batch): (8, 19200) Loss: 0.3978332518041134\n",
            "(Epoch, batch): (8, 25600) Loss: 0.41455965876579287\n",
            "(Epoch, batch): (8, 32000) Loss: 0.4015676745772362\n",
            "(Epoch, batch): (8, 38400) Loss: 0.40423290818929675\n",
            "(Epoch, batch): (8, 44800) Loss: 0.4003475284576416\n",
            "Epoch: 8 Accuracy: 80.96\n",
            "(Epoch, batch): (9, 6400) Loss: 0.39132690206170084\n",
            "(Epoch, batch): (9, 12800) Loss: 0.3843516609072685\n",
            "(Epoch, batch): (9, 19200) Loss: 0.3807983049750328\n",
            "(Epoch, batch): (9, 25600) Loss: 0.36927193000912667\n",
            "(Epoch, batch): (9, 32000) Loss: 0.3682571691274643\n",
            "(Epoch, batch): (9, 38400) Loss: 0.3729690165817738\n",
            "(Epoch, batch): (9, 44800) Loss: 0.3618113878369331\n",
            "Epoch: 9 Accuracy: 81.2\n",
            "(Epoch, batch): (10, 6400) Loss: 0.3587715269625187\n",
            "(Epoch, batch): (10, 12800) Loss: 0.3567439042031765\n",
            "(Epoch, batch): (10, 19200) Loss: 0.3685751946270466\n",
            "(Epoch, batch): (10, 25600) Loss: 0.36354454725980756\n",
            "(Epoch, batch): (10, 32000) Loss: 0.36885567955672743\n",
            "(Epoch, batch): (10, 38400) Loss: 0.3679234316945076\n",
            "(Epoch, batch): (10, 44800) Loss: 0.35885335117578504\n",
            "Epoch: 10 Accuracy: 82.36\n",
            "(Epoch, batch): (11, 6400) Loss: 0.3730456229299307\n",
            "(Epoch, batch): (11, 12800) Loss: 0.3698935204744339\n",
            "(Epoch, batch): (11, 19200) Loss: 0.3520933772623539\n",
            "(Epoch, batch): (11, 25600) Loss: 0.3464119888842106\n",
            "(Epoch, batch): (11, 32000) Loss: 0.3374302007257938\n",
            "(Epoch, batch): (11, 38400) Loss: 0.3623484642058611\n",
            "(Epoch, batch): (11, 44800) Loss: 0.3349055707454681\n",
            "Epoch: 11 Accuracy: 82.66\n",
            "(Epoch, batch): (12, 6400) Loss: 0.32129519917070865\n",
            "(Epoch, batch): (12, 12800) Loss: 0.3140928326547146\n",
            "(Epoch, batch): (12, 19200) Loss: 0.3299127934873104\n",
            "(Epoch, batch): (12, 25600) Loss: 0.33163313686847684\n",
            "(Epoch, batch): (12, 32000) Loss: 0.3469581885635853\n",
            "(Epoch, batch): (12, 38400) Loss: 0.33732117131352424\n",
            "(Epoch, batch): (12, 44800) Loss: 0.34670222833752634\n",
            "Epoch: 12 Accuracy: 83.32\n",
            "(Epoch, batch): (13, 6400) Loss: 0.3203541810065508\n",
            "(Epoch, batch): (13, 12800) Loss: 0.31751395083963874\n",
            "(Epoch, batch): (13, 19200) Loss: 0.32484619222581385\n",
            "(Epoch, batch): (13, 25600) Loss: 0.3299622455984354\n",
            "(Epoch, batch): (13, 32000) Loss: 0.3124870710074902\n",
            "(Epoch, batch): (13, 38400) Loss: 0.32621385231614114\n",
            "(Epoch, batch): (13, 44800) Loss: 0.33994730934500694\n",
            "Epoch: 13 Accuracy: 82.4\n",
            "(Epoch, batch): (14, 6400) Loss: 0.3116457752883434\n",
            "(Epoch, batch): (14, 12800) Loss: 0.3057968108355999\n",
            "(Epoch, batch): (14, 19200) Loss: 0.29294411920011043\n",
            "(Epoch, batch): (14, 25600) Loss: 0.3107027365267277\n",
            "(Epoch, batch): (14, 32000) Loss: 0.3081903683394194\n",
            "(Epoch, batch): (14, 38400) Loss: 0.3080022548884153\n",
            "(Epoch, batch): (14, 44800) Loss: 0.29793165177106856\n",
            "Epoch: 14 Accuracy: 82.58\n",
            "(Epoch, batch): (15, 6400) Loss: 0.2337508761137724\n",
            "(Epoch, batch): (15, 12800) Loss: 0.2153533773869276\n",
            "(Epoch, batch): (15, 19200) Loss: 0.21689651992172002\n",
            "(Epoch, batch): (15, 25600) Loss: 0.20950907707214356\n",
            "(Epoch, batch): (15, 32000) Loss: 0.1906072773039341\n",
            "(Epoch, batch): (15, 38400) Loss: 0.1985639014840126\n",
            "(Epoch, batch): (15, 44800) Loss: 0.19872160360217095\n",
            "Epoch: 15 Accuracy: 86.52\n",
            "(Epoch, batch): (16, 6400) Loss: 0.19066786665469407\n",
            "(Epoch, batch): (16, 12800) Loss: 0.19099401898682117\n",
            "(Epoch, batch): (16, 19200) Loss: 0.1796730213984847\n",
            "(Epoch, batch): (16, 25600) Loss: 0.1736398770287633\n",
            "(Epoch, batch): (16, 32000) Loss: 0.1823121163621545\n",
            "(Epoch, batch): (16, 38400) Loss: 0.19067193683236838\n",
            "(Epoch, batch): (16, 44800) Loss: 0.17504410151392222\n",
            "Epoch: 16 Accuracy: 86.98\n",
            "(Epoch, batch): (17, 6400) Loss: 0.17429696265608072\n",
            "(Epoch, batch): (17, 12800) Loss: 0.15727115213871\n",
            "(Epoch, batch): (17, 19200) Loss: 0.17946789406239985\n",
            "(Epoch, batch): (17, 25600) Loss: 0.17087782971560955\n",
            "(Epoch, batch): (17, 32000) Loss: 0.1695231157913804\n",
            "(Epoch, batch): (17, 38400) Loss: 0.1751102152094245\n",
            "(Epoch, batch): (17, 44800) Loss: 0.1601163089647889\n",
            "Epoch: 17 Accuracy: 87.52\n",
            "(Epoch, batch): (18, 6400) Loss: 0.16176371216773988\n",
            "(Epoch, batch): (18, 12800) Loss: 0.16707006871700286\n",
            "(Epoch, batch): (18, 19200) Loss: 0.16341728005558254\n",
            "(Epoch, batch): (18, 25600) Loss: 0.16696595467627048\n",
            "(Epoch, batch): (18, 32000) Loss: 0.15704204514622688\n",
            "(Epoch, batch): (18, 38400) Loss: 0.16054235205054282\n",
            "(Epoch, batch): (18, 44800) Loss: 0.1723741589859128\n",
            "Epoch: 18 Accuracy: 87.28\n",
            "(Epoch, batch): (19, 6400) Loss: 0.17300518855452537\n",
            "(Epoch, batch): (19, 12800) Loss: 0.1412340560927987\n",
            "(Epoch, batch): (19, 19200) Loss: 0.16704701844602823\n",
            "(Epoch, batch): (19, 25600) Loss: 0.1565074396133423\n",
            "(Epoch, batch): (19, 32000) Loss: 0.15690155152231455\n",
            "(Epoch, batch): (19, 38400) Loss: 0.15052429994568228\n",
            "(Epoch, batch): (19, 44800) Loss: 0.16867828767746687\n",
            "Epoch: 19 Accuracy: 87.52\n",
            "(Epoch, batch): (20, 6400) Loss: 0.15191896330565213\n",
            "(Epoch, batch): (20, 12800) Loss: 0.15041869098320604\n",
            "(Epoch, batch): (20, 19200) Loss: 0.15068940337747336\n",
            "(Epoch, batch): (20, 25600) Loss: 0.15184305565431713\n",
            "(Epoch, batch): (20, 32000) Loss: 0.15551197454333304\n",
            "(Epoch, batch): (20, 38400) Loss: 0.1538845258206129\n",
            "(Epoch, batch): (20, 44800) Loss: 0.1633973726630211\n",
            "Epoch: 20 Accuracy: 86.74\n",
            "(Epoch, batch): (21, 6400) Loss: 0.15524667154997587\n",
            "(Epoch, batch): (21, 12800) Loss: 0.14958510544151069\n",
            "(Epoch, batch): (21, 19200) Loss: 0.1538963009044528\n",
            "(Epoch, batch): (21, 25600) Loss: 0.13994678616523742\n",
            "(Epoch, batch): (21, 32000) Loss: 0.15138529546558857\n",
            "(Epoch, batch): (21, 38400) Loss: 0.15056006111204623\n",
            "(Epoch, batch): (21, 44800) Loss: 0.1468100705742836\n",
            "Epoch: 21 Accuracy: 87.58\n",
            "(Epoch, batch): (22, 6400) Loss: 0.14713418543338774\n",
            "(Epoch, batch): (22, 12800) Loss: 0.14543365670368075\n",
            "(Epoch, batch): (22, 19200) Loss: 0.14316602669656275\n",
            "(Epoch, batch): (22, 25600) Loss: 0.14181645218282937\n",
            "(Epoch, batch): (22, 32000) Loss: 0.1281927975639701\n",
            "(Epoch, batch): (22, 38400) Loss: 0.1411061978712678\n",
            "(Epoch, batch): (22, 44800) Loss: 0.12919633191078903\n",
            "Epoch: 22 Accuracy: 87.88\n",
            "(Epoch, batch): (23, 6400) Loss: 0.1318902475759387\n",
            "(Epoch, batch): (23, 12800) Loss: 0.1354937993362546\n",
            "(Epoch, batch): (23, 19200) Loss: 0.14534911569207906\n",
            "(Epoch, batch): (23, 25600) Loss: 0.1338061025366187\n",
            "(Epoch, batch): (23, 32000) Loss: 0.13783811572939159\n",
            "(Epoch, batch): (23, 38400) Loss: 0.14168765815906226\n",
            "(Epoch, batch): (23, 44800) Loss: 0.1416694876179099\n",
            "Epoch: 23 Accuracy: 87.58\n",
            "(Epoch, batch): (24, 6400) Loss: 0.1366248257830739\n",
            "(Epoch, batch): (24, 12800) Loss: 0.13972021717578173\n",
            "(Epoch, batch): (24, 19200) Loss: 0.13727044396102428\n",
            "(Epoch, batch): (24, 25600) Loss: 0.1383424523472786\n",
            "(Epoch, batch): (24, 32000) Loss: 0.12278010871261358\n",
            "(Epoch, batch): (24, 38400) Loss: 0.14113413464277982\n",
            "(Epoch, batch): (24, 44800) Loss: 0.1390067096799612\n",
            "Epoch: 24 Accuracy: 87.42\n",
            "(Epoch, batch): (25, 6400) Loss: 0.13978987649083138\n",
            "(Epoch, batch): (25, 12800) Loss: 0.13097017651423812\n",
            "(Epoch, batch): (25, 19200) Loss: 0.15054271511733533\n",
            "(Epoch, batch): (25, 25600) Loss: 0.13433597855269908\n",
            "(Epoch, batch): (25, 32000) Loss: 0.1367174569889903\n",
            "(Epoch, batch): (25, 38400) Loss: 0.1507528310455382\n",
            "(Epoch, batch): (25, 44800) Loss: 0.1275132756307721\n",
            "Epoch: 25 Accuracy: 87.86\n",
            "(Epoch, batch): (26, 6400) Loss: 0.1380147996172309\n",
            "(Epoch, batch): (26, 12800) Loss: 0.12880150094628334\n",
            "(Epoch, batch): (26, 19200) Loss: 0.13929947718977928\n",
            "(Epoch, batch): (26, 25600) Loss: 0.1393358039110899\n",
            "(Epoch, batch): (26, 32000) Loss: 0.1343818524107337\n",
            "(Epoch, batch): (26, 38400) Loss: 0.13634451668709516\n",
            "(Epoch, batch): (26, 44800) Loss: 0.12656803756952287\n",
            "Epoch: 26 Accuracy: 87.44\n",
            "(Epoch, batch): (27, 6400) Loss: 0.1334254358522594\n",
            "(Epoch, batch): (27, 12800) Loss: 0.13434348234906793\n",
            "(Epoch, batch): (27, 19200) Loss: 0.12460959060117602\n",
            "(Epoch, batch): (27, 25600) Loss: 0.13166557207703591\n",
            "(Epoch, batch): (27, 32000) Loss: 0.12837551604956388\n",
            "(Epoch, batch): (27, 38400) Loss: 0.13094965796917676\n",
            "(Epoch, batch): (27, 44800) Loss: 0.13804286727681755\n",
            "Epoch: 27 Accuracy: 87.32\n",
            "(Epoch, batch): (28, 6400) Loss: 0.138161357101053\n",
            "(Epoch, batch): (28, 12800) Loss: 0.1369671350158751\n",
            "(Epoch, batch): (28, 19200) Loss: 0.1290995009429753\n",
            "(Epoch, batch): (28, 25600) Loss: 0.1297749545238912\n",
            "(Epoch, batch): (28, 32000) Loss: 0.139056380353868\n",
            "(Epoch, batch): (28, 38400) Loss: 0.13134385287761688\n",
            "(Epoch, batch): (28, 44800) Loss: 0.11796391360461712\n",
            "Epoch: 28 Accuracy: 87.36\n",
            "(Epoch, batch): (29, 6400) Loss: 0.13613654601387679\n",
            "(Epoch, batch): (29, 12800) Loss: 0.13152428921312093\n",
            "(Epoch, batch): (29, 19200) Loss: 0.13141834005713462\n",
            "(Epoch, batch): (29, 25600) Loss: 0.13149718821048736\n",
            "(Epoch, batch): (29, 32000) Loss: 0.1269852275028825\n",
            "(Epoch, batch): (29, 38400) Loss: 0.1309907180070877\n",
            "(Epoch, batch): (29, 44800) Loss: 0.14027931697666646\n",
            "Epoch: 29 Accuracy: 87.16\n",
            "(Epoch, batch): (30, 6400) Loss: 0.12938400864601135\n",
            "(Epoch, batch): (30, 12800) Loss: 0.13691073670983314\n",
            "(Epoch, batch): (30, 19200) Loss: 0.13031581919640303\n",
            "(Epoch, batch): (30, 25600) Loss: 0.13146179903298616\n",
            "(Epoch, batch): (30, 32000) Loss: 0.13909561313688756\n",
            "(Epoch, batch): (30, 38400) Loss: 0.1352596740424633\n",
            "(Epoch, batch): (30, 44800) Loss: 0.13564323119819163\n",
            "Epoch: 30 Accuracy: 87.82\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk4E2K0BKXSy"
      },
      "source": [
        "Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OXEdfa4HrMX",
        "outputId": "be87d1b9-4dec-421b-fb9f-08b8c8e7f71b"
      },
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    images, labels = data\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    outputs = net(images)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    c = (predicted == labels).squeeze()\n",
        "\n",
        "    for i in range(labels.shape[0]):\n",
        "      label = labels[i]\n",
        "      class_correct[label] += c[i].item()\n",
        "      class_total[label] += 1\n",
        "\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  \n",
        "print(\"Accuracy of the network on test images: %d\" % (100 * correct / total))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on test images: 86\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}